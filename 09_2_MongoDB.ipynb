{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB Atlas\n",
    "\n",
    "To use MongoDB Atlas, you must first deploy a cluster. To get started head over to Atlas here: [quick start](https://www.mongodb.com/docs/atlas/getting-started/).\n",
    "Create an Atlas database and create an Atlas Search Index to search vectors.\n",
    "\n",
    "Follow below MongoDB Atlas guide\n",
    "- [Create new cluster](https://www.mongodb.com/docs/atlas/tutorial/create-new-cluster/)\n",
    "- [Connect to database](https://www.mongodb.com/docs/atlas/driver-connection/)\n",
    "- [Create an Atlas Vector Search Index](https://www.mongodb.com/docs/atlas/atlas-vector-search/create-index/)\n",
    "- [Create Index Fields](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#std-label-avs-types-vector-search)\n",
    "\n",
    "### benefits?\n",
    "- use mongoDB itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25 pymongo langchain langchain-mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY\n",
    "# ... KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Your connection string should use following format:\n",
    "mongodb+srv://<username>:<password>@<clusterName>.<hostname>.mongodb.net\n",
    "\"\"\"\n",
    "MONGODB_ATLAS_CLUSTER_URI = os.environ[\"MONGODB_ATLAS_CLUSTER_URI\"]\n",
    "\n",
    "# Connect to your Atlas cluster\n",
    "client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)\n",
    "# Define collection and index name\n",
    "DB_NAME = \"langchain_db\"\n",
    "COLLECTION_NAME = \"test\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "\n",
    "db_collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexes\n",
    "\n",
    "create atlas index fields.\n",
    "\n",
    "[Create Index Fields](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#std-label-avs-types-vector-search)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"numDimensions\": 4096,\n",
    "      \"path\": \"embedding\",\n",
    "      \"similarity\": \"dotProduct\",\n",
    "      \"type\": \"vector\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Korea is a beautiful country to visit in the spring.'), Document(page_content='The best time to visit Korea is in the fall.'), Document(page_content='Best way to find bug is using unit test.'), Document(page_content='Python is a great programming language for beginners.'), Document(page_content='Sung Kim is a great teacher.')]\n",
      "batch_size: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "sample_text = [\n",
    "    \"Korea is a beautiful country to visit in the spring.\",\n",
    "    \"The best time to visit Korea is in the fall.\",\n",
    "    \"Best way to find bug is using unit test.\",\n",
    "    \"Python is a great programming language for beginners.\",\n",
    "    \"Sung Kim is a great teacher.\",\n",
    "]\n",
    "\n",
    "splits = RecursiveCharacterTextSplitter().create_documents(sample_text)\n",
    "\n",
    "print(splits)\n",
    "\n",
    "vectorstore = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=splits,\n",
    "    collection=db_collection,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_collection.find_one({\"text\":\"Hello, new sentence\"}) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_collection.find_one({\"text\":splits[0].page_content}) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 125\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=1000, chunk_overlap=100, language=Language.HTML\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "vectorstore = MongoDBAtlasVectorSearch(\n",
    "    collection=db_collection,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "unique_splits = [\n",
    "    split for split in splits if not db_collection.find_one({\"text\":split.page_content})\n",
    "]\n",
    "print(len(unique_splits))\n",
    "\n",
    "# 3. Embed & indexing if it's not in the vector store\n",
    "if len(unique_splits) > 0:\n",
    "    MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=unique_splits,\n",
    "    collection=MONGODB_COLLECTION,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m search_result \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to find problems in code?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(search_result)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msearch_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpage_content[:\u001b[38;5;241m100\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Query the retriever\n",
    "search_result = retriever.invoke(\"How to find problems in code?\")\n",
    "print(search_result)\n",
    "print(search_result[0].page_content[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hybrid search\n",
    "\n",
    "We will carry out a hybrid search that combines BM25 and vector search techniques. This process will unfold in two separate stages: first within LangChain, and then via a query in MongoDB. Moreover, we will apply reciprocal rank fusion to merge results from different search methods into a single, unified outcome.\n",
    "\n",
    "Add additional **Search Index** to MongoDB Atlas with following definition. To enable keyword search.\n",
    "```json\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"dynamic\": false,\n",
    "    \"fields\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "# Using Langchain \n",
    "\n",
    "\n",
    "# 1. initializing retrievers\n",
    "vectorstore = MongoDBAtlasVectorSearch(\n",
    "    collection=db_collection,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
    ")\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "\n",
    "# set weights for reciprocal rank fusion\n",
    "hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever], weights=[0.4,0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"<p id='13' style='font-size:16px'>introduced bugs immediately. Several bug-finding techni-<br>ques could be used, including code inspections, unit testing,<br>and the use of static analysis tools. Since these steps would<br>be taken right after a code change was made, the developer<br>would still retain the full mental context of the change. This<br>holds promise for reducing the time required to find<br>software bugs and reducing the time that bugs stay resident<br>in software before removal.</p><br>\", metadata={'_id': {'$oid': '66500f045d6a17e9c9316f7d'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='42' style='font-size:16px'>determine which kinds of function return values must be<br>checked. For example, if the return value of foo was always<br>verified in the previous project history but was not verified<br>in the current source code, it is very suspicious. Livshits and<br>Zimmermann combine software repository mining and<br>dynamic analysis to discover common use patterns and<br>code patterns that are likely errors in Java applications [25].<br>Similarly, PR-Miner mines common call sequences from a<br>code snapshot and then marks all noncommon call patterns<br>as potential bugs [24].</p><br><p id='43' style='font-size:16px'>These approaches are similar to change classification<br>since they use project specific patterns to determine latent<br>software bugs. However, the mining is limited to specific<br>patterns such as return types or call sequences and hence<br>limits the type of latent bugs that can be identified.</p><br>\", metadata={'_id': {'$oid': '66500f045d6a17e9c9316f91'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='39' style='font-size:20px'>2.2 Mining Buggy Patterns</p><br><p id='40' style='font-size:16px'>One thread of research attempts to find buggy or clean code<br>patterns in the history of development of a software project.</p><br><p id='41' style='font-size:16px'>Williams and Hollingsworth use project histories to<br>improve existing bug-finding tools [51]. Using a return<br>value without first checking its validity may be a latent bug.<br>In practice, this approach leads to many false positives as<br>typical code has many locations where return values are<br>used without checking. To remove the false positives,<br>Williams and Hollingsworth use project histories to</p><br>\", metadata={'_id': {'$oid': '66500f045d6a17e9c9316f90'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content='<br>the latent bugs. In contrast, efforts that detect faults by<br>analyzing the source code by using static or dynamic<br>analysis techniques can identify specific kinds of bugs in the<br>software, though generally with high rates of false positives.<br>Common techniques include type checking, deadlock<br>detection, and pattern recognition [8], [26], [48].</p><br>', metadata={'_id': {'$oid': '66500f045d6a17e9c9316f88'}, 'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='37' style='font-size:16px'>Khoshgoftaar and Allen have proposed a model to list<br>modules according to software quality factors such as<br>future fault density [15], [16], [21]. The inputs to the model<br>are software complexity metrics such as LOC, number of<br>unique operators, and cyclomatic complexity. A stepwise<br>multiregression is then performed to find weights for each<br>factor [15], [16]. Mockus and Weiss predict risky modules in<br>software by using a regression algorithm and change<br>measures such as the number of systems touched, the<br>number of modules touched, the number of lines of added<br>code, and the number of modification requests [33].</p><br><p id='38' style='font-size:16px'>Pan et al. use metrics computed over software slice data<br>in conjunction with machine learning algorithms to find<br>bug-prone software files or functions [41]. Their approach<br>tries to find faults in the whole code, while our approach<br>focuses on file changes.</p><br>\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='48' style='font-size:16px'>Similar in spirit to change classification is work that<br>classifies bug reports or software maintenance requests [3],<br>[10]. In this research, keywords in bug reports or change<br>requests are extracted and used as features to train a<br>machine learning classifier. The goal of the classification is<br>to place a bug report into a specific category or to find the<br>developer best suited to fix a bug. This work, along with<br>change classification, highlights the potential of using<br>machine learning techniques in software engineering. If</p><p id='51' style='font-size:16px'>an existing concern such as assigning bugs to developers<br>can be recast as a classification problem, then it is possible<br>to leverage the large collection of data stored in bug<br>tracking and SCM systems.</p><br><p id='52' style='font-size:20px'>2.4 Text Classification</p><br>\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content=\"<p id='47' style='font-size:16px'>Research that categorizes or associates source code with<br>other documents (traceability recovery) is similar to ours in<br>that it gathers terms from source code and then uses<br>learning or statistical approaches to find associated docu-<br>ments [2], [42]. For example, Maletic et al. [28], [29]<br>extracted all features available in the source code via Latent<br>Semantic Analysis (LSA) and then used this data to cluster<br>software and to create relationships between the source<br>code and other related software project documents. In a<br>similar vein, Kuhn et al. use partial terms from the source<br>code to cluster the code to detect abnormal module<br>structures [20]. Antoniol et al. use stochastic modeling<br>and Bayesian classification for traceability recovery [2].<br>Their work differs from ours in that they only use features<br>from the source code, while our change classification learns\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'}), Document(page_content='<br>the prediction granularity of change classification is much<br>finer, file level changes , which, for the projects that we<br>analyzed, average 20 lines of code (LOCs) per change. This<br>is significant since developers need to examine an order of<br>magnitude smaller number of LOCs to find latent bugs with<br>the change classification approach. Gyimothy et al. use<br>release-based classes for prediction, where a release is an<br>accumulation of many versions, while our change classifi-<br>cation applies to the changes between successive individual<br>versions. This allows change classification to be used in an<br>ongoing daily manner instead of just for releases which<br>occur on months-long time scales.</p><br>', metadata={'total_pages': 16, 'type': 'html', 'split': 'none', 'title': 'Classifying Software Changes: Clean or Buggy?'})]\n"
     ]
    }
   ],
   "source": [
    "# 2. Query using hybrid retriever\n",
    "docs = hybrid_retriever.get_relevant_documents(\"How to find prblems in code?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MongoDB query\n",
    "\n",
    "# One of the great things about the MongoDB Atlas vector store is the variety of queries we can use.\n",
    "# Perfrom hybrid search using MongoDB query.\n",
    "\n",
    "# The reciprocal rank score is calculated as below\n",
    "# 1.0/{document position in the results + vector or full-text penalty + constant value}\n",
    "\n",
    "def hybrid_search(client, query):\n",
    "    vector_penalty = 4\n",
    "    keyword_penalty = 6\n",
    "    return client.aggregate(\n",
    "        [\n",
    "            {\n",
    "                # $vectorSearch stage to search the embedding field for the query specified as vector embeddings in the queryVector field of the query.\n",
    "                # The query specifies a search for up to 100 nearest neighbors and limit the results to 20 documents only. This stage returns the sorted documents from the semantic search in the results.\n",
    "                \"$vectorSearch\": {\n",
    "                    \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": UpstageEmbeddings(\n",
    "                        model=\"solar-embedding-1-large\"\n",
    "                    ).embed_query(query),\n",
    "                    \"numCandidates\": 10,\n",
    "                    \"limit\": 5,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $group stage to group all the documents in the results from the semantic search in a field named docs.\n",
    "                \"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}\n",
    "            },\n",
    "            {\n",
    "                # $unwind stage to unwind the array of documents in the docs field and store the position of the document in the results array in a field named rank.\n",
    "                \"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}\n",
    "            },\n",
    "            {\n",
    "                # $addFields stage to add a new field named vs_score that contains the reciprocal rank score for each document in the results.\n",
    "                # Here, reciprocal rank score is calculated by dividing 1.0 by the sum of rank, the vector_penalty weight, and a constant value of 1.\n",
    "                \"$addFields\": {\n",
    "                    \"vs_score\": {\n",
    "                        \"$divide\": [1.0, {\"$add\": [\"$rank\", vector_penalty, 1]}]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $project stage to include only the following fields in the results: vs_score, _id, title, text\n",
    "                \"$project\": {\n",
    "                    \"vs_score\": 1,\n",
    "                    \"_id\": \"$docs._id\",\n",
    "                    \"title\": \"$docs.title\",\n",
    "                    \"text\": \"$docs.text\",\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $unionWith stage to combine the results from the preceding stages with the results of the following stages in the sub-pipeline\n",
    "                \"$unionWith\": {\n",
    "                    \"coll\": COLLECTION_NAME,\n",
    "                    \"pipeline\": [\n",
    "                        {\n",
    "                            # $search stage to search for movies that contain the query in the text field. This stage returns the sorted documents from the keyword search in the results.\n",
    "                            \"$search\": {\n",
    "                                \"index\": \"text\",\n",
    "                                \"phrase\": {\"query\": query, \"path\": \"text\"},\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            # $limit stage to limit the output to 15 results only.\n",
    "                            \"$limit\": 15\n",
    "                        },\n",
    "                        {\n",
    "                            # $group stage to group all the documents from the keyword search in a field named docs.\n",
    "                            \"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}\n",
    "                        },\n",
    "                        {\n",
    "                            # $unwind stage to unwind the array of documents in the docs field and store the position of the document in the results array in a field named rank.\n",
    "                            \"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}\n",
    "                        },\n",
    "                        {\n",
    "                            # $addFields stage to add a new field named kws_score that contains the reciprocal rank score for each document in the results.\n",
    "                            # Here, reciprocal rank score is calculated by dividing 1.0 by the sum of the value of rank, the full_text penalty weight, and a constant value of 1.\n",
    "                            \"$addFields\": {\n",
    "                                \"kws_score\": {\n",
    "                                    \"$divide\": [\n",
    "                                        1.0,\n",
    "                                        {\"$add\": [\"$rank\", keyword_penalty, 1]},\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            # $project stage to include only the following fields in the results: kws_score, _id, title, text\n",
    "                            \"$project\": {\n",
    "                                \"kws_score\": 1,\n",
    "                                \"_id\": \"$docs._id\",\n",
    "                                \"title\": \"$docs.title\",\n",
    "                                \"text\": \"$docs.text\",\n",
    "                            }\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $project stage to include only the following fields in the results: _id, title, text, vs_score, kws_score\n",
    "                \"$project\": {\n",
    "                    \"title\": 1,\n",
    "                    \"vs_score\": {\"$ifNull\": [\"$vs_score\", 0]},\n",
    "                    \"kws_score\": {\"$ifNull\": [\"$kws_score\", 0]},\n",
    "                    \"text\": 1,\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                # $project stage to add a field named score that contains the sum of vs_score and kws_score to the results.\n",
    "                \"$project\": {\n",
    "                    \"score\": {\"$add\": [\"$kws_score\", \"$vs_score\"]},\n",
    "                    \"title\": 1,\n",
    "                    \"vs_score\": 1,\n",
    "                    \"kws_score\": 1,\n",
    "                    \"text\": 1\n",
    "                }\n",
    "            },\n",
    "            # $sort stage to sort the results by score in descending order.\n",
    "            {\"$sort\": {\"score\": -1}},\n",
    "            #   $limit stage to limit the output to 10 results only.\n",
    "            {\"$limit\": 10},\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p id='13' style='font-size:16px'>introduced bugs immediately. Several bug-finding techni-<br>ques could be used, including code inspections, unit testing,<br>and the use of static analysis tools. Since these steps would<br>be taken right after a code change was made, the developer<br>would still retain the full mental context of the change. This<br>holds promise for reducing the time required to find<br>software bugs and reducing the time that bugs stay resident<br>in software before removal.</p><br> 0.2 \n",
      "\n",
      "<p id='42' style='font-size:16px'>determine which kinds of function return values must be<br>checked. For example, if the return value of foo was always<br>verified in the previous project history but was not verified<br>in the current source code, it is very suspicious. Livshits and<br>Zimmermann combine software repository mining and<br>dynamic analysis to discover common use patterns and<br>code patterns that are likely errors in Java applications [25].<br>Similarly, PR-Miner mines common call sequences from a<br>code snapshot and then marks all noncommon call patterns<br>as potential bugs [24].</p><br><p id='43' style='font-size:16px'>These approaches are similar to change classification<br>since they use project specific patterns to determine latent<br>software bugs. However, the mining is limited to specific<br>patterns such as return types or call sequences and hence<br>limits the type of latent bugs that can be identified.</p><br> 0.16666666666666666 \n",
      "\n",
      "<p id='39' style='font-size:20px'>2.2 Mining Buggy Patterns</p><br><p id='40' style='font-size:16px'>One thread of research attempts to find buggy or clean code<br>patterns in the history of development of a software project.</p><br><p id='41' style='font-size:16px'>Williams and Hollingsworth use project histories to<br>improve existing bug-finding tools [51]. Using a return<br>value without first checking its validity may be a latent bug.<br>In practice, this approach leads to many false positives as<br>typical code has many locations where return values are<br>used without checking. To remove the false positives,<br>Williams and Hollingsworth use project histories to</p><br> 0.14285714285714285 \n",
      "\n",
      "<br>the latent bugs. In contrast, efforts that detect faults by<br>analyzing the source code by using static or dynamic<br>analysis techniques can identify specific kinds of bugs in the<br>software, though generally with high rates of false positives.<br>Common techniques include type checking, deadlock<br>detection, and pattern recognition [8], [26], [48].</p><br> 0.125 \n",
      "\n",
      "<p id='35' style='font-size:16px'>Brun and Ernst [5] use two classification algorithms to<br>find hidden code errors. Using Ernstâ€™s Daikon dynamic<br>invariant detector, invariant features are extracted from<br>code with known errors and with errors removed. They<br>train SVM and decision tree classifiers by using the<br>extracted features and then classify invariants in the source<br>code as either fault invariant or non-fault-invariant. The<br>fault-invariant information is used to find hidden errors in<br>the source code. The reported classification accuracy is<br>10.6 percent on average (9 percent for C and 12.2 percent for<br>Java), with a classification precision of 21.6 percent on<br>average (10 percent for C and 33.2 percent for Java), and the<br>best classification precision (with the top 80 relevant<br>invariants) of 52 percent on average (45 percent for C and<br>59 percent for Java). The classified fault invariants guide 0.1111111111111111 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = hybrid_search(db_collection, \"How to find prblems in code?\")\n",
    "for doc in result:\n",
    "    print(doc['text'], doc['score'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
