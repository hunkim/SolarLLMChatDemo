{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary, Writing, Translation\n",
    "Common applications of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU langchain-upstage  requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# set UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_text = \"\"\"\n",
    "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\n",
    "\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "which encompasses depthwise scaling and continued pretraining.\n",
    "In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "DUS does not require complex changes to train and inference efficiently. \n",
    "We show experimentally that DUS is simple yet effective \n",
    "in scaling up high-performance LLMs from small ones. \n",
    "Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "a variant fine-tuned for instruction-following capabilities, \n",
    "surpassing Mixtral-8x7B-Instruct. \n",
    "SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "promoting broad access and application in the LLM field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please do {action} on the following text: \n",
    "    ---\n",
    "    TEXT: {text}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSOLAR 10.7B, a 10.7 billion parameter large language model, demonstrates superior performance in various NLP tasks. The model uses depth up-scaling (DUS) method, which includes depthwise scaling and continued pretraining. DUS is a simple yet effective way to scale up high-performance LLMs without requiring complex changes to training and inference. Additionally, SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, is available. SOLAR 10.7B is publicly available under the Apache 2.0 license.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"action\": \"three line summarize\", \"text\": solar_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We created a new computer program called SOLAR 10.7B that is very good at understanding and speaking different languages. This program can do many things, like answering questions, writing stories, and even helping with homework.\\n\\nTo make this program better, we used a special way called \"depth up-scaling\" or DUS. This means we made the program bigger and stronger, but we did it in a simple way that is easy to understand and use. We also made sure that the program can learn new things on its own, just like a child learning from their experiences.\\n\\nWe showed that our simple way of making the program bigger and stronger works really well. It is much better than other ways that use more complicated ideas.\\n\\nWe also made a special version of the program called SOLAR 10.7B-Instruct. This version is even better at following instructions and doing what it\\'s told. It\\'s so good that it\\'s better than other programs that use even more complicated ideas.\\n\\nNow, everyone can use our program for free and make new things with it. We hope that our program will help people learn and do cool things with computers.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"action\": \"Please rewrite this so that children and grandma can understand it easily.\", \"text\": solar_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "enko_translation = ChatUpstage(model=\"solar-1-mini-translate-enko\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = chat_prompt | enko_translation | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리는 107억 개의 매개변수를 가진 대규모 언어 모델 (LLM)인 SOLAR 10.7B를 소개합니다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"text\": \"\"\"We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "demonstrating superior performance in various natural language processing (NLP) tasks. \"\"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"we present a method for scaling LLMs called depth up-scaling (DUS)\"),\n",
    "        (\"ai\", \"DUS라는 방법에 대해 알려줄께. DUS는 LLMs를 확장하는 방법이야.\"),\n",
    "        (\"human\", \"In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently.\"),\n",
    "        (\"ai\", \"다른 LLM 확장 방법과는 달리, DUS는 복잡한 변경 사항 없이 효율적으로 훈련하고 추론할 수 있어.\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = style_prompt | enko_translation | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리는 107억 개의 매개 변수를 가진 대규모 언어 모델(LLM)인 SOLAR 10.7B를 소개하며, 다양한 자연어 처리(NLP) 작업에서 우수한 성능을 보여줘.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"text\": \"\"\"We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. \"\"\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
